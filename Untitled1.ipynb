{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df432ad8-95ae-42e2-8334-272f37f31547",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0991cb27-a128-45ff-b52c-7b625e478b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "# tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "print(physical_devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12425d93-1ff6-4042-a183-6240341099d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.experimental.set_virtual_device_configuration(\n",
    "    physical_devices[0],\n",
    "    [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024*4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40229e08-5691-4388-9e68-1835e8c9cf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "image_size = 256\n",
    "batch_size = 20\n",
    "channels = 3\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c642466-3ad4-44bc-82af-588145f15985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15557 files belonging to 23 classes.\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    \"train\",\n",
    "    shuffle=True,\n",
    "    image_size=(image_size, image_size),\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f6913ed-79aa-4db0-a6d9-680e42afbf7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_partitions_tf(ds, train_split=0.8, val_split=0.2,shuffle=True, shuffle_size=10000):\n",
    "    ds_size = len(ds)\n",
    "    \n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(shuffle_size, seed=12)\n",
    "    \n",
    "    train_size = int(train_split * ds_size)\n",
    "    val_size = int(val_split * ds_size)\n",
    "\n",
    "    train_ds = ds.take(train_size)\n",
    "\n",
    "    val_ds = ds.skip(train_size).take(val_size)\n",
    "\n",
    "    return train_ds, val_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "332879fb-174e-444e-a651-3a3b7c34edfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, val_ds = get_dataset_partitions_tf(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9168cf60-e360-42f2-bf54-4bb02e2d07a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_ds.cache().shuffle(10000).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "val_ds = val_ds.cache().shuffle(10000).prefetch(buffer_size=tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc7d7e02-04fc-4c77-bab8-3a12f6f4c1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load VGG16 model\n",
    "input_shape = (image_size, image_size, channels)\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db18d088-37f4-4055-a278-ea308ad93322",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "n_classes = 23  # Adjust this based on the number of classes in your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3474ac6d-d99d-4f87-9e11-245791bc45d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = Sequential([\n",
    "    keras.layers.experimental.preprocessing.Resizing(image_size, image_size),\n",
    "    keras.layers.experimental.preprocessing.Rescaling(1.0/255),\n",
    "    keras.layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\"),\n",
    "    keras.layers.experimental.preprocessing.RandomRotation(0.2),\n",
    "    base_model,\n",
    "    GlobalAveragePooling2D(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(n_classes, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "12d2c4ef-da07-48ad-972e-b197096657c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\.conda\\envs\\tf\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=Adam(lr=0.0001),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c99172ab-6952-44dc-b904-000f4405d0ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "622/622 [==============================] - 277s 426ms/step - loss: 2.7420 - accuracy: 0.1836 - val_loss: 2.6926 - val_accuracy: 0.2026\n",
      "Epoch 2/10\n",
      "622/622 [==============================] - 301s 484ms/step - loss: 2.7362 - accuracy: 0.1862 - val_loss: 2.6884 - val_accuracy: 0.2006\n",
      "Epoch 3/10\n",
      "622/622 [==============================] - 1013s 2s/step - loss: 2.7319 - accuracy: 0.1834 - val_loss: 2.6854 - val_accuracy: 0.2045\n",
      "Epoch 4/10\n",
      "622/622 [==============================] - 1001s 2s/step - loss: 2.7274 - accuracy: 0.1820 - val_loss: 2.6835 - val_accuracy: 0.2058\n",
      "Epoch 5/10\n",
      "622/622 [==============================] - 1000s 2s/step - loss: 2.7271 - accuracy: 0.1844 - val_loss: 2.6845 - val_accuracy: 0.2029\n",
      "Epoch 6/10\n",
      "622/622 [==============================] - 1003s 2s/step - loss: 2.7257 - accuracy: 0.1839 - val_loss: 2.6792 - val_accuracy: 0.2045\n",
      "Epoch 7/10\n",
      "622/622 [==============================] - 1001s 2s/step - loss: 2.7284 - accuracy: 0.1865 - val_loss: 2.6778 - val_accuracy: 0.2055\n",
      "Epoch 8/10\n",
      "622/622 [==============================] - 1002s 2s/step - loss: 2.7240 - accuracy: 0.1879 - val_loss: 2.6734 - val_accuracy: 0.2039\n",
      "Epoch 9/10\n",
      "622/622 [==============================] - 1003s 2s/step - loss: 2.7161 - accuracy: 0.1862 - val_loss: 2.6705 - val_accuracy: 0.2029\n",
      "Epoch 10/10\n",
      "213/622 [=========>....................] - ETA: 9:29 - loss: 2.7278 - accuracy: 0.1819"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=epochs,\n",
    "    verbose=1,\n",
    "    validation_data=val_ds\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb48421c-95f3-460e-a424-dd2be3433d75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
